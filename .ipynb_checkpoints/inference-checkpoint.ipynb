{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = 'weights/TOKENIZER.pkl'\n",
    "model_weights = 'weights/DESCRIPTION_SIMILARITY.h5'\n",
    "book_features_file = 'weights/BOOK_FEATURES.npz'\n",
    "\n",
    "max_length = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tensorflow.keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights/DESCRIPTION_SIMILARITY.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[0;32m      3\u001b[0m re_tokenizer \u001b[38;5;241m=\u001b[39m RegexpTokenizer(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_model' is not defined"
     ]
    }
   ],
   "source": [
    "model = load_model('weights/DESCRIPTION_SIMILARITY.h5', compile=False)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "re_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "with open(tokenizer_path, 'rb') as fp:\n",
    "    tokenizer = pickle.load(fp)\n",
    "\n",
    "Fmodel = tf.keras.models.load_model(model_weights)\n",
    "\n",
    "book_ids, book_features = np.load(book_features_file, allow_pickle=True).values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GENERATE METADATA FOR DATABASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### UPDATE BOOK DATA ###################\n",
    "\n",
    "# df_book_metadata = pd.read_csv('data/books_v2.csv')\n",
    "# df_book_metadata = df_book_metadata.dropna(subset=['description'])\n",
    "# df_book_metadata = df_book_metadata[:2000]\n",
    "# df_book_metadata = df_book_metadata.reset_index(drop=True)\n",
    "# df_book_metadata = df_book_metadata[['bookId', 'title', 'author', 'description', 'coverImg']]\n",
    "\n",
    "# rating_arr = np.random.choice([1, 2, 3, 4, 5], size=len(df_book_metadata), p=[0.1, 0.1, 0.4, 0.2, 0.2])\n",
    "# df_book_metadata['rating'] = rating_arr\n",
    "# df_book_metadata.to_csv('data/books.csv', index=False)\n",
    "\n",
    "\n",
    "################### USER DATA ###################\n",
    "\n",
    "# user_Data_dict = {}\n",
    "# user_Data_dict['userId'] = np.arange(1, 2001).tolist()\n",
    "# user_Data_dict['bookId'] = pd.read_csv('data/books.csv')['bookId'].tolist()\n",
    "\n",
    "# # # generate 2000 random latutude and longitude in srilanka\n",
    "# lat_min, long_min = 6.9271, 79.8612\n",
    "# lat_max, long_max = 7.9271, 80.0255\n",
    "# user_Data_dict['location'] = np.random.uniform(low=[lat_min, long_min], high=[lat_max, long_max], size=(2000, 2)).tolist()\n",
    "# user_Data_dict['Occupied'] = np.random.choice([0, 1], size=2000, p=[0.6, 0.4]).tolist()\n",
    "\n",
    "# df_user_data = pd.DataFrame(user_Data_dict)\n",
    "# df_user_data.to_csv('data/users.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    df_book_metadata = pd.read_csv('data/books_v2.csv')\n",
    "    df_book_metadata = df_book_metadata.dropna(subset=['description'])\n",
    "    df_book_metadata = df_book_metadata[:2000]\n",
    "    \n",
    "    bookIds = df_book_metadata['bookId'].values\n",
    "    titles = df_book_metadata['title'].values\n",
    "    authors = df_book_metadata['author'].values\n",
    "    coverImgs = df_book_metadata['coverImg'].values\n",
    "    ratings = df_book_metadata['rating'].values\n",
    "\n",
    "    df_user = pd.read_csv('data/users.csv')\n",
    "    userIds = df_user['userId'].values\n",
    "    bookIds = df_user['bookId'].values\n",
    "    locations = df_user['location'].values\n",
    "    Occupied = df_user['Occupied'].values\n",
    "\n",
    "\n",
    "\n",
    "    book_metadata = {bookId : {\n",
    "                    'title': title, \n",
    "                    'author': author, \n",
    "                    'coverImg': coverImg, \n",
    "                    'rating': rating\n",
    "                    } for bookId, title, author, coverImg, rating in zip(bookIds, titles, authors, coverImgs, ratings)}\n",
    "                    \n",
    "    user_metadata = {bookId : {\n",
    "                        'userId': userId, \n",
    "                        'location': location, \n",
    "                        'Occupied': Occupied\n",
    "                        } for userId, bookId, location, Occupied in zip(userIds, bookIds, locations, Occupied)} \n",
    "    \n",
    "    return book_metadata, user_metadata\n",
    "\n",
    "def lemmatization(lemmatizer,sentence):\n",
    "    lem = [lemmatizer.lemmatize(k) for k in sentence]\n",
    "    return [k for k in lem if k]\n",
    "\n",
    "def remove_stop_words(stopwords_list,sentence):\n",
    "    return [k for k in sentence if k not in stopwords_list]\n",
    "\n",
    "def preprocess_one(description):\n",
    "    description = description.lower()\n",
    "    remove_punc = re_tokenizer.tokenize(description) # Remove puntuations\n",
    "    remove_num = [re.sub('[0-9]', '', i) for i in remove_punc] # Remove Numbers\n",
    "    remove_num = [i for i in remove_num if len(i)>0] # Remove empty strings\n",
    "    lemmatized = lemmatization(lemmatizer,remove_num) # Word Lemmatization\n",
    "    remove_stop = remove_stop_words(stopwords_list,lemmatized) # remove stop words\n",
    "    updated_description = ' '.join(remove_stop)\n",
    "    return updated_description\n",
    "\n",
    "def preprocessed_data(descriptions):\n",
    "    updated_descriptions = []\n",
    "    if isinstance(descriptions, np.ndarray) or isinstance(descriptions, list):\n",
    "        updated_descriptions = [preprocess_one(description) for description in descriptions]\n",
    "    elif isinstance(descriptions, np.str_)  or isinstance(descriptions, str):\n",
    "        updated_descriptions = [preprocess_one(descriptions)]\n",
    "\n",
    "    return np.array(updated_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_metadata, user_metadata = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    R = 6371 # metres\n",
    "    phi1 = np.radians(lat1)\n",
    "    phi2 = np.radians(lat2)\n",
    "    delta_phi = np.radians(lat2-lat1)\n",
    "    delta_lambda = np.radians(lon2-lon1)\n",
    "\n",
    "    a = np.sin(delta_phi/2) * np.sin(delta_phi/2) + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda/2) * np.sin(delta_lambda/2)\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "\n",
    "    d = R * c\n",
    "    return d\n",
    "\n",
    "def extract_similar_books(\n",
    "                        description, \n",
    "                        user_location,\n",
    "                        max_distance=50,\n",
    "                        top_n=30\n",
    "                        ):\n",
    "    description = preprocessed_data(description)\n",
    "    d_seq = tokenizer.texts_to_sequences(description)\n",
    "    d_pad = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "                                                        d_seq,\n",
    "                                                        maxlen=max_length,\n",
    "                                                        padding='pre',\n",
    "                                                        truncating='pre'\n",
    "                                                        )\n",
    "    fpred = Fmodel.predict(d_pad)\n",
    "    fpred = fpred.reshape(1, -1)\n",
    "    sim = cosine_similarity(fpred, book_features)\n",
    "    sim = sim.reshape(-1)\n",
    "    sim = np.argsort(sim)[::-1]\n",
    "    sim_book_ids =  book_ids[sim[:top_n]]\n",
    "    sim_book_data = {}\n",
    "    for book_id in sim_book_ids:\n",
    "        try:\n",
    "            book_rating = int(book_metadata[book_id]['rating'])\n",
    "            rec_book_location = eval(user_metadata[book_id]['location'])\n",
    "            rec_book_occupied = int(user_metadata[book_id]['Occupied'])\n",
    "            distance = haversine_distance(user_location[0], user_location[1], rec_book_location[0], rec_book_location[1])\n",
    "            distance = round(distance, 2)\n",
    "            if (book_rating >= 3) and (distance <= max_distance) and (rec_book_occupied == 0):\n",
    "                sim_book_data[book_id] = book_metadata[book_id] \n",
    "                sim_book_data[book_id]['distance'] = distance\n",
    "                sim_book_data[book_id]['rating'] = book_rating\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    sim_book_data_reponse = []\n",
    "    for book_id, book_data in sim_book_data.items():\n",
    "        sim_book_data_reponse.append({\n",
    "                                    \"bookId\": f\"{book_id}\",\n",
    "                                    \"title\": f\"{book_data['title']}\",\n",
    "                                    \"author\": f\"{book_data['author']}\",\n",
    "                                    \"coverImg\": f\"{book_data['coverImg']}\",\n",
    "                                    \"rating\": f\"{book_data['rating']}\",\n",
    "                                    \"distance\": f\"{book_data['distance']} Km\"\n",
    "                                    })\n",
    "\n",
    "\n",
    "    return sim_book_data_reponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bookId': '10626594-the-scorpio-races',\n",
       "  'title': 'The Scorpio Races',\n",
       "  'author': 'Maggie Stiefvater (Goodreads Author), Carina Jansson (Translator)',\n",
       "  'coverImg': 'https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1461209661l/10626594.jpg',\n",
       "  'rating': '4',\n",
       "  'distance': '5.88 Km'},\n",
       " {'bookId': '3165162-percy-jackson-and-the-olympians',\n",
       "  'title': 'Percy Jackson and the Olympians',\n",
       "  'author': 'Rick Riordan (Goodreads Author)',\n",
       "  'coverImg': 'https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1388181482l/3165162.jpg',\n",
       "  'rating': '4',\n",
       "  'distance': '4.65 Km'},\n",
       " {'bookId': '7735333-matched',\n",
       "  'title': 'Matched',\n",
       "  'author': 'Ally Condie (Goodreads Author)',\n",
       "  'coverImg': 'https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1367706191l/7735333.jpg',\n",
       "  'rating': '3',\n",
       "  'distance': '9.84 Km'},\n",
       " {'bookId': '144349.The_Burning_Bridge',\n",
       "  'title': 'The Burning Bridge',\n",
       "  'author': 'John Flanagan',\n",
       "  'coverImg': 'https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1440471172l/144349._SY475_.jpg',\n",
       "  'rating': '4',\n",
       "  'distance': '19.77 Km'},\n",
       " {'bookId': '20821111-the-young-elites',\n",
       "  'title': 'The Young Elites',\n",
       "  'author': 'Marie Lu (Goodreads Author)',\n",
       "  'coverImg': 'https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1407318399l/20821111.jpg',\n",
       "  'rating': '3',\n",
       "  'distance': '19.37 Km'},\n",
       " {'bookId': '20801439-circle-of-five',\n",
       "  'title': 'Circle of Five',\n",
       "  'author': 'Jan Raymond (Goodreads Author)',\n",
       "  'coverImg': 'https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1392257498l/20801439.jpg',\n",
       "  'rating': '4',\n",
       "  'distance': '15.84 Km'}]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description = \"WINNING MEANS FAME AND FORTUNE.LOSING MEANS CERTAIN DEATH.THE HUNGER GAMES HAVE BEGUN. . . .In the ruins of a place once known as North America lies the nation of Panem, a shining Capitol surrounded by twelve outlying districts. The Capitol is harsh and cruel and keeps the districts in line by forcing them all to send one boy and once girl between the ages of twelve and eighteen to participate in the annual Hunger Games, a fight to the death on live TV.Sixteen-year-old Katniss Everdeen regards it as a death sentence when she steps forward to take her sister's place in the Games. But Katniss has been close to dead beforeâ€”and survival, for her, is second nature. Without really meaning to, she becomes a contender. But if she is to win, she will have to start making choices that weight survival against humanity and life against love.\"\n",
    "user_location = [6.9271, 79.8612]\n",
    "\n",
    "extract_similar_books(description, user_location, max_distance=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4b521e29a846470c96e928a1c4aafac58a12234cdaa98f9ca60bc431873fee6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
